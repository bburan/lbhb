'''
Interface for reading tone-evoked ABR data generated by psiexperiment

This supports merging across aggregate files. Right now there's no support for
reading from the raw (i.e., untrial_filtered) data. Maybe eventually. It wouldn't be
hard to add, but would need support for trial_filter specs as well as ensuring that we
pull out enough pre/post samples for proper trial_filtering.
'''
import ast
import functools
from pathlib import Path
import os.path
import shutil
import re
from glob import glob

from tqdm import tqdm
import bcolz
import numpy as np
import pandas as pd
from scipy import signal

from . import util

from psi.data.io import abr


MAXSIZE = 1024


def load_analysis(base_folder):
    search_pattern = os.path.join(base_folder, '*-analyzed.txt')
    result = [load_abr_analysis(f) for f in glob(search_pattern)]

    names = ['analyzer', 'start', 'end', 'filter_lb', 'filter_ub', 'frequency']
    freq, th, info, data = zip(*result)

    keys = []
    for f, i in zip(freq, info):
        key = tuple(i[n] for n in names[:-1]) + (f,)
        keys.append(key)

    index = pd.MultiIndex.from_tuples(keys, names=names)
    threshold = pd.Series(th, index=index, name='threshold')
    peaks = pd.concat(data, keys=keys, names=names)
    peaks.sort_index(inplace=True)
    return threshold, peaks


def cleanup_analysis(result, info, simplify, analyzer):
    if analyzer is not None:
        result = result.xs(analyzer, level='analyzer')
    if not simplify:
        return result
    remove = []
    for level in result.index.names:
        if level not in info:
            values = result.index.get_level_values(level)
            if len(set(values)) == 1:
                remove.append(level)
    result.reset_index(remove, drop=True, inplace=True)
    return result


class ABRDataset:

    def __init__(self, experiments):
        self.experiments = experiments

    @classmethod
    def from_folder(cls, folder):
        folder = Path(folder)
        experiments = [ABRExperiment(f) for f in folder.glob('*abr')]
        return cls(experiments)

    def get_thresholds(self, columns, simplify=True, analyzer=None):
        keys = self.get_info(columns, 'list')
        thresholds = [e.all_thresholds for e in self.experiments]
        result = pd.concat(thresholds, keys=keys, names=columns)
        return cleanup_analysis(result, columns, simplify, analyzer)

    def get_waves(self, columns, simplify=True, analyzer=None):
        keys = self.get_info(columns, 'list')
        waves = [e.all_waves for e in self.experiments]
        result = pd.concat(waves, keys=keys, names=columns)
        return cleanup_analysis(result, columns, simplify, analyzer)

    def get_epochs(self, columns, **kwargs):
        keys = self.get_info(columns, 'list')
        epochs = [e.get_mean_epochs(**kwargs) for e in self.experiments]
        return pd.concat(epochs, keys=keys, names=columns)

    def find_experiments(self, **kwargs):
        if 'date' in kwargs:
            if isinstance(kwargs['date'], pd.Timestamp):
                kwargs['date'] = kwargs['date'].date()
        cols = kwargs.keys()
        vals = tuple(kwargs.values())
        matches = []
        for e in self.experiments:
            if e.get_info(cols) == vals:
                matches.append(e)
        return matches

    def get_experiment(self, **kwargs):
        experiments = self.find_experiments(**kwargs)
        if len(experiments) > 1:
            raise ValueError('More than one match')
        elif len(experiments) < 1:
            raise ValueError('No experiment matching criteria')
        else:
            return experiments[0]

    def get_info(self, columns, flavor='dataframe'):
        info = [e.get_info(columns) for e in self.experiments]
        if flavor == 'dataframe':
            return pd.DataFrame(info, columns=columns)
        elif flavor == 'index':
            return pd.MultiIndex.from_tuples(info, names=columns)
        elif flavor == 'list':
            return info

    def __getitem__(self, slice):
        return self.experiments[slice]

    def __iter__(self):
        return iter(self.experiments)


class ABRExperiment:

    def __init__(self, base_folder):
        self._base_folder = base_folder
        self._fh = abr.load(base_folder)

    def get_info(self, columns, verify_integrity=False, flavor='tuple'):
        info = util.parse_filename(self._base_folder)

        # Check to see if we need to actually load the trial log. If not, keep
        # it simple and fast.
        result = []
        for c in columns:
            # special hack for ordering
            if c == 'ordering':
                ordering = self._fh.erp_metadata[c].iloc[0]
                if ordering == 'interleaved':
                    l1, l2 = self._fh.erp_metadata['level'].iloc[:2]
                    ordering = 'plateau' if l1 == l2 else 'ramp'
                result.append(ordering)
            elif c not in info:
                data = self._fh.erp_metadata[c]
                if verify_integrity and len(np.unique(data)) != 1:
                    m = f'Column {c} has more than one unique value'
                    raise ValueError(m)
                result.append(data.iloc[0])
            else:
                result.append(info[c])

        if flavor == 'tuple':
            return tuple(result)
        elif flavor == 'series':
            return pd.Series(result, index=columns)
        else:
            raise ValueError(f'Unsupported flavor {flavor}')

    @property
    @functools.lru_cache(maxsize=MAXSIZE)
    def analyzed_data(self):
        return load_analysis(self._base_folder)

    @property
    @functools.lru_cache(maxsize=MAXSIZE)
    def all_thresholds(self):
        return self.analyzed_data[0]

    @property
    @functools.lru_cache(maxsize=MAXSIZE)
    def thresholds(self):
        return self.all_thresholds.groupby('frequency').mean()

    @property
    @functools.lru_cache(maxsize=MAXSIZE)
    def all_waves(self):
        return self.analyzed_data[1]

    @property
    @functools.lru_cache(maxsize=MAXSIZE)
    def waves(self):
        return self.all_waves.groupby(['frequency', 'level']).mean()

    def get_epochs(self, *args, **kwargs):
        return self._fh.get_epochs(*args, **kwargs)

    def get_epochs_filtered(self, *args, **kwargs):
        return self._fh.get_epochs_filtered(*args, **kwargs)

    def get_random_segments(self, *args, **kwargs):
        return self._fh.get_random_segments(*args, **kwargs)

    def get_random_segments_filtered(self, *args, **kwargs):
        return self._fh.get_random_segments_filtered(*args, **kwargs)

    def _get_mean(self, fn, *args, **kwargs):
        epochs = fn(*args, **kwargs)
        reject_threshold, = self.get_info(['reject_threshold'])
        m = np.abs(epochs) <= reject_threshold
        m = m.all(axis=1)
        return epochs.loc[m].groupby(['frequency', 'level', 'polarity']).mean() \
            .groupby(['frequency', 'level']).mean()

    def get_mean_epochs_filtered(self, *args, **kwargs):
        return self._get_mean(self.get_epochs_filtered)

    def get_mean_epochs(self, *args, **kwargs):
        return self._get_mean(self.get_epochs)


MERGE_PATTERN = \
    r'\g<date>-* ' \
    r'\g<experimenter> ' \
    r'\g<animal> ' \
    r'\g<ear> ' \
    r'\g<note> ' \
    r'\g<experiment>*'


ABR_ANALYZED_FILE_PATTERN = \
    r'ABR (?P<start>-?\d+\.\d+)ms ' + \
    r'to (?P<end>-?\d+\.\d+)ms ' + \
    r'(with (?P<filter_lb>\d+)Hz ' + \
    r'to (?P<filter_ub>\d+)Hz ' + \
    r'filter )?average waveforms' + \
    r'-(?P<frequency>\d+\.\d+)kHz-' + \
    r'((?P<analyzer>\w+)-)?' + \
    r'analyzed.txt'


P_ABR_ANALYZED_FILE_PATTERN = re.compile(ABR_ANALYZED_FILE_PATTERN)
#WAVEFORM_FILE = 'ABR -1.0ms to 9.0ms with 300Hz to 3000Hz filter average waveforms.csv'

def load_abr_analysis(filename):
    rename = {
        'Level': 'level',
        '1msec Avg': 'baseline',
        '1msec StDev': 'baseline_std',
        'P1 Latency': 'p1_latency',
        'P1 Amplitude': 'p1_amplitude',
        'N1 Latency': 'n1_latency',
        'N1 Amplitude': 'n1_amplitude',
        'P2 Latency': 'p2_latency',
        'P2 Amplitude': 'p2_amplitude',
        'N2 Latency': 'n2_latency',
        'N2 Amplitude': 'n2_amplitude',
        'P3 Latency': 'p3_latency',
        'P3 Amplitude': 'p3_amplitude',
        'N3 Latency': 'n3_latency',
        'N3 Amplitude': 'n3_amplitude',
        'P4 Latency': 'p4_latency',
        'P4 Amplitude': 'p4_amplitude',
        'N4 Latency': 'n4_latency',
        'N4 Amplitude': 'n4_amplitude',
        'P5 Latency': 'p5_latency',
        'P5 Amplitude': 'p5_amplitude',
        'N5 Latency': 'n5_latency',
        'N5 Amplitude': 'n5_amplitude',
    }

    th_match = re.compile('Threshold \(dB SPL\): ([\w.-]+)')
    freq_match = re.compile('Frequency \(kHz\): ([\d.]+)')
    with open(filename) as fh:
        for line in fh:
            # Parse the threshold string
            if line.startswith('Threshold'):
                th_string = th_match.search(line).group(1)
                if th_string == 'None':
                    th = -np.inf
                elif th_string == 'inf':
                    th = np.inf
                elif th_string == '-inf':
                    th = -np.inf
                else:
                    th = float(th_string)

            if line.startswith('Frequency'):
                freq = float(freq_match.search(line).group(1))*1e3

            if line.startswith('NOTE'):
                break

        data = pd.io.parsers.read_csv(fh, sep='\t')
        data.rename(columns=rename, inplace=True)
        #data['frequency'] = freq*1e3
        #data['threshold'] = th

    base, head = os.path.split(filename)
    info = P_ABR_ANALYZED_FILE_PATTERN.match(head).groupdict()
    info['start'] = float(info['start'])*1e-3
    info['end'] = float(info['end'])*1e-3
    parse = lambda x: float(x) if x is not None else 0
    info['filter_lb'] = parse(info['filter_lb'])
    info['filter_ub'] = parse(info['filter_ub'])
    del info['frequency']

    m = data['level'] >= th
    data = data.loc[m]
    data = data[list(rename.values())] \
        .set_index('level', verify_integrity=True) \
        .sort_index()
    return freq, th, info, data


def load_abr_analysis_for_experiments(abr_experiments, progressbar=True):
    abr_data = []
    filenames = []
    for filename in abr_experiments['filename']:
        search_pattern = os.path.join(filename, '*-analyzed.txt')
        filenames.extend(glob(search_pattern))

    iterator = tqdm(filenames) if progressbar else filenames
    peaks = [load_abr_analysis(filename) for filename in iterator]
    return pd.concat(peaks)
